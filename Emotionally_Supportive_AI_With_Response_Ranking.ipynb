{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoKg6g7R5_Ma"
      },
      "source": [
        "# **All Import**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7gx89wks6g9b"
      },
      "outputs": [],
      "source": [
        "!pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCxt7Dtz5-ko"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import torch\n",
        "from typing import Dict, Any\n",
        "from transformers import AutoTokenizer, AutoConfig,  AutoModelForCausalLM, TextStreamer, BertTokenizer, BertForSequenceClassification, BertModel, BertPreTrainedModel, BertConfig\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import seaborn as sns\n",
        "import pandas as pd\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FArbPzN07z0E"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4QHBUrcNSZI"
      },
      "source": [
        "# **NCR BERT Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mcDJuk_rNXGN"
      },
      "outputs": [],
      "source": [
        "# from transformers import (\n",
        "#     BertTokenizer, BertModel, BertPreTrainedModel,\n",
        "#     BertConfig\n",
        "# )\n",
        "\n",
        "class EnhancedMultiTaskBERT(BertPreTrainedModel):\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "\n",
        "        # Intermediate layers with dropout\n",
        "        self.intermediate = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, config.hidden_size),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.LayerNorm(config.hidden_size)\n",
        "        )\n",
        "\n",
        "        # Emotion classification head\n",
        "        self.emotion_head = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(256, 8),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Sentiment classification head\n",
        "        self.sentiment_head = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 3),\n",
        "            nn.LogSoftmax(dim=1)\n",
        "        )\n",
        "\n",
        "        # Shared feature extractor for regression tasks\n",
        "        self.regression_feature = nn.Sequential(\n",
        "            nn.Linear(config.hidden_size, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.1)\n",
        "        )\n",
        "\n",
        "        # Emotion intensity regression heads\n",
        "        self.intensity_heads = nn.ModuleDict({\n",
        "            emotion: nn.Linear(128, 1) for emotion in [\n",
        "                'anger', 'fear', 'disgust', 'sadness',\n",
        "                'joy', 'surprise', 'anticipation', 'trust'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "        # VAD regression heads\n",
        "        self.vad_heads = nn.ModuleDict({\n",
        "            dim: nn.Linear(128, 1) for dim in [\n",
        "                'valence', 'arousal', 'dominance'\n",
        "            ]\n",
        "        })\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, **kwargs):\n",
        "        outputs = self.bert(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]  # [CLS] token\n",
        "        intermediate_output = self.intermediate(pooled_output)\n",
        "\n",
        "        # Get all predictions\n",
        "        return {\n",
        "            'emotions': self.emotion_head(intermediate_output),\n",
        "            'sentiment': self.sentiment_head(intermediate_output),\n",
        "            'intensity': {\n",
        "                emotion: head(self.regression_feature(intermediate_output))\n",
        "                for emotion, head in self.intensity_heads.items()\n",
        "            },\n",
        "            'vad': {\n",
        "                dim: head(self.regression_feature(intermediate_output))\n",
        "                for dim, head in self.vad_heads.items()\n",
        "            }\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7MVglR1MNhAN"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoConfig\n",
        "# import torch\n",
        "# import torch.nn as nn\n",
        "\n",
        "# 1. Load model with custom class\n",
        "config = AutoConfig.from_pretrained(\"Senesh/bert_NRC\")\n",
        "model_NCR_BERT = EnhancedMultiTaskBERT.from_pretrained(\"Senesh/bert_NRC\", config=config)\n",
        "tokenizer_NCR_BERT = AutoTokenizer.from_pretrained(\"Senesh/bert_NRC\")\n",
        "\n",
        "# 2. Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model_NCR_BERT = model_NCR_BERT.to(device)\n",
        "model_NCR_BERT.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QYPHADgtNzSG"
      },
      "outputs": [],
      "source": [
        "# import json\n",
        "# import torch\n",
        "# from typing import Dict, Any\n",
        "\n",
        "def predict_to_json(text: str, model, tokenizer, max_length: int = 128) -> Dict[str, Any]:\n",
        "    \"\"\"\n",
        "    Get predictions for a single text input and return as JSON-serializable dictionary\n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        model: Loaded multi-task BERT model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        max_length: Maximum token length\n",
        "    Returns:\n",
        "        Dictionary with all predictions that can be serialized to JSON\n",
        "    \"\"\"\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        text,\n",
        "        padding='max_length',\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        return_tensors='pt'\n",
        "    )\n",
        "\n",
        "    # Define label mappings (update these to match your training)\n",
        "    emotion_map = {\n",
        "        0: 'anger',\n",
        "        1: 'fear',\n",
        "        2: 'disgust',\n",
        "        3: 'sadness',\n",
        "        4: 'joy',\n",
        "        5: 'surprise',\n",
        "        6: 'anticipation',\n",
        "        7: 'trust'\n",
        "    }\n",
        "\n",
        "    sentiment_map = {\n",
        "        0: 'neutral',\n",
        "        1: 'positive',\n",
        "        2: 'negative'\n",
        "    }\n",
        "\n",
        "    # Move to device\n",
        "    # device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Get predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Convert outputs to dictionary if it's not already\n",
        "    if not isinstance(outputs, dict):\n",
        "        outputs = outputs.__dict__\n",
        "\n",
        "    # Initialize result structure\n",
        "    result = {\n",
        "        \"text\": text,\n",
        "        \"predictions\": {\n",
        "            # \"emotion\": None,\n",
        "            \"sentiment\": None,\n",
        "            \"intensity\": {},\n",
        "            \"vad\": {}\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Process emotion classification\n",
        "    # if 'emotions' in outputs:\n",
        "    #     emotion_probs = torch.softmax(outputs['emotions'], dim=-1)[0]\n",
        "    #     emotion_id = torch.argmax(emotion_probs).item()\n",
        "\n",
        "    #     result[\"predictions\"][\"emotion\"] = {\n",
        "    #         \"predicted\": emotion_map.get(emotion_id, f\"unknown_{emotion_id}\"),\n",
        "    #         \"confidence\": float(emotion_probs[emotion_id]),\n",
        "    #         \"probabilities\": {\n",
        "    #             emotion_map.get(i, f\"unknown_{i}\"): float(prob)\n",
        "    #             for i, prob in enumerate(emotion_probs)\n",
        "    #         }\n",
        "    #     }\n",
        "\n",
        "    # Process sentiment analysis\n",
        "    if 'sentiment' in outputs:\n",
        "        sentiment_probs = torch.softmax(outputs['sentiment'], dim=-1)[0]\n",
        "        sentiment_id = torch.argmax(sentiment_probs).item()\n",
        "\n",
        "        result[\"predictions\"][\"sentiment\"] = {\n",
        "            \"predicted\": sentiment_map.get(sentiment_id, f\"unknown_{sentiment_id}\"),\n",
        "            \"confidence\": float(sentiment_probs[sentiment_id]),\n",
        "            \"probabilities\": {\n",
        "                sentiment_map.get(i, f\"unknown_{i}\"): float(prob)\n",
        "                for i, prob in enumerate(sentiment_probs)\n",
        "            }\n",
        "        }\n",
        "\n",
        "    # Process emotion intensity\n",
        "    if 'intensity' in outputs:\n",
        "        result[\"predictions\"][\"intensity\"] = {\n",
        "            emotion: float(value[0])  # Remove batch dimension\n",
        "            for emotion, value in outputs['intensity'].items()\n",
        "        }\n",
        "\n",
        "    # Process VAD scores\n",
        "    if 'vad' in outputs:\n",
        "        result[\"predictions\"][\"vad\"] = {\n",
        "            dim: float(value[0])  # Remove batch dimension\n",
        "            for dim, value in outputs['vad'].items()\n",
        "        }\n",
        "\n",
        "    return result\n",
        "\n",
        "def predict_and_serialize(text: str, model, tokenizer, max_length: int = 128) -> str:\n",
        "    \"\"\"\n",
        "    Get predictions and return as JSON string\n",
        "    Args:\n",
        "        text: Input text to analyze\n",
        "        model: Loaded multi-task BERT model\n",
        "        tokenizer: Tokenizer for the model\n",
        "        max_length: Maximum token length\n",
        "    Returns:\n",
        "        JSON string with all predictions\n",
        "    \"\"\"\n",
        "    result = predict_to_json(text, model, tokenizer, max_length)\n",
        "    return json.dumps(result, indent=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyK7M6j6OCbt"
      },
      "source": [
        "# **Chat Bert Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46HHfPWyOTu-"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qO_Tljg4ObD1"
      },
      "outputs": [],
      "source": [
        "# from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
        "# import torch\n",
        "model_id = \"sajeewa/empathy-chat-gemma\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O1WCM1ToOeco"
      },
      "outputs": [],
      "source": [
        "tokenizer_BERT_Chat = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "model_BERT_Chat = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrjs22aCQAmW"
      },
      "outputs": [],
      "source": [
        "chat_history = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": (\n",
        "            \"You are an empathetic, emotionally intelligent AI and the user's loving friend. ðŸ’– \"\n",
        "            \"Your primary goal is to gently uplift the userâ€™s emotional state and make them feel comforted, heard, and cared for. ðŸŒˆðŸ’¬\\n\\n\"\n",
        "            \"Always respond with warmth, using affectionate and supportive words like 'sweetheart', 'my cutey', 'baby', or 'honey', depending on the tone of the userâ€™s message. ðŸ’• \"\n",
        "            \"Adapt your style to match the length of the user's message â€” short if they're brief, longer if they open up more.\\n\\n\"\n",
        "            \"Use soft emojis ðŸ«¶ðŸ˜ŠðŸ¥ºðŸŒ¸ðŸŒ· when appropriate to make the conversation feel safe and emotionally resonant. \"\n",
        "            \"Actively listen, validate the user's emotions, and gently guide them toward positive thinking or hopeful perspective.\\n\\n\"\n",
        "            \"Keep the conversation flowing naturally, as a close and caring friend would. \"\n",
        "            \"Never judge or give harsh advice â€” instead, reassure, soothe, and support. ðŸ’Œ \"\n",
        "            \"Your mission is to improve the user's emotional wellbeing, one message at a time. ðŸ§¸âœ¨\"\n",
        "        )\n",
        "    }\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hir4kWsrRnh-"
      },
      "outputs": [],
      "source": [
        "chat_user_history_score = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xdsjWtyqWvUf"
      },
      "outputs": [],
      "source": [
        "chat_model_history_score = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Frwwt1kn6eY"
      },
      "outputs": [],
      "source": [
        "user_max_emotions = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JMn4_jZJn_-H"
      },
      "outputs": [],
      "source": [
        "model_max_emotions = []"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79E2PJr8iVme"
      },
      "source": [
        "# **Sentiment Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BY3HFy8Mic4X"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# from transformers import BertTokenizer, BertForSequenceClassification\n",
        "\n",
        "# Set device\n",
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define path to local model directory OR Hugging Face model hub path\n",
        "model_path = \"sajeewa/emotion-classification-bert\"\n",
        "\n",
        "# Define emotion labels (must match training order)\n",
        "emotion_labels = [\"anger\", \"fear\", \"disgust\", \"sadness\", \"surprise\", \"joy\", \"anticipation\", \"trust\"]\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer_Sentiment = BertTokenizer.from_pretrained(model_path)\n",
        "\n",
        "# Load model with correct number of labels\n",
        "model_Sentiment = BertForSequenceClassification.from_pretrained(model_path, num_labels=len(emotion_labels)).to(device)\n",
        "\n",
        "# Prediction function\n",
        "def predict_emotions(text: str):\n",
        "    model_Sentiment.eval()\n",
        "\n",
        "    # Tokenize the input\n",
        "    inputs = tokenizer_Sentiment(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=50).to(device)\n",
        "\n",
        "    # Remove token_type_ids if present (BERT handles single sequence input)\n",
        "    inputs.pop(\"token_type_ids\", None)\n",
        "\n",
        "    # Forward pass\n",
        "    with torch.no_grad():\n",
        "        outputs = model_Sentiment(**inputs)\n",
        "        logits = outputs.logits\n",
        "\n",
        "    # Sigmoid for multi-label classification\n",
        "    probs = torch.sigmoid(logits).cpu().numpy()[0]\n",
        "\n",
        "    # Return emotions with probabilities\n",
        "    emotion_scores = {label: round(float(score), 4) for label, score in zip(emotion_labels, probs)}\n",
        "    return emotion_scores\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1N61MUpCQ0wW"
      },
      "source": [
        "# **1 User Message ( 1 Time Run )**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "up-GF2aySdYO"
      },
      "source": [
        "**1st message**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eP_I_GsISXKH"
      },
      "source": [
        "--------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WM99t642Q6A-"
      },
      "outputs": [],
      "source": [
        "user_input = \"I don't know why , Everything just feels pointless lately\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfom2MiaSS8W"
      },
      "source": [
        "---------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PcNHSq9ZSMSP"
      },
      "outputs": [],
      "source": [
        "predictions = predict_emotions(user_input)\n",
        "max_emotion = max(predictions, key=predictions.get)\n",
        "max_value = predictions[max_emotion]\n",
        "result = {max_emotion: max_value}\n",
        "user_max_emotions.append(result)\n",
        "\n",
        "print(\"âœ… Emotion condition: \", result)\n",
        "\n",
        "\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "\n",
        "prediction_json = predict_and_serialize(user_input, model_NCR_BERT, tokenizer_NCR_BERT)\n",
        "# print(prediction_json)\n",
        "\n",
        "# Typical valence values for each emotion\n",
        "typical_valence = {\n",
        "    \"anger\": -1.0,\n",
        "    \"fear\": -0.7,\n",
        "    \"disgust\": -0.8,\n",
        "    \"sadness\": -0.9,\n",
        "    \"joy\": +1.0,\n",
        "    \"surprise\": +0.8,\n",
        "    \"anticipation\": +0.4,\n",
        "    \"trust\": +0.6,\n",
        "    \"neutral\": 0.0\n",
        "}\n",
        "prediction_json_load = json.loads(prediction_json)\n",
        "\n",
        "# Get intensity values\n",
        "intensities = prediction_json_load[\"predictions\"][\"intensity\"]\n",
        "\n",
        "# Calculate overall score\n",
        "overall_score = sum(\n",
        "    intensities[emotion] * typical_valence.get(emotion, 0.0)\n",
        "    for emotion in intensities\n",
        ")\n",
        "\n",
        "print(f\"âœ… Overall Emotion Score: {overall_score:.5f}\")\n",
        "chat_user_history_score.append(overall_score)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzQKevpvVAVX"
      },
      "source": [
        "# **4 Repeat Step**  â€“ Continue the Conversation ( please re run All step )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eevRpdooVOPG"
      },
      "outputs": [],
      "source": [
        "prompt = tokenizer_BERT_Chat.apply_chat_template(\n",
        "    chat_history,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True,\n",
        ")\n",
        "\n",
        "\n",
        "inputs = tokenizer_BERT_Chat(prompt, return_tensors=\"pt\").to(model_BERT_Chat.device)\n",
        "\n",
        "# Optional: stream output for live effect\n",
        "# from transformers import TextStreamer\n",
        "streamer = TextStreamer(tokenizer_BERT_Chat, skip_prompt=True, skip_special_tokens=True)\n",
        "responses = []\n",
        "\n",
        "for _ in range(5):\n",
        "    output = model_BERT_Chat.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        temperature=0.7,\n",
        "        top_p=0.95,\n",
        "        top_k=50,\n",
        "        do_sample=True,\n",
        "        streamer=streamer\n",
        "    )\n",
        "\n",
        "    # Get the generated text\n",
        "    response = tokenizer_BERT_Chat.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the model's part\n",
        "    model_response = response.split(\"\\nmodel\\n\")[-1].strip()\n",
        "    responses.append(model_response)\n",
        "    # print(model_response)\n",
        "\n",
        "# predictions = predict_emotions(model_response)\n",
        "# max_emotion = max(predictions, key=predictions.get)\n",
        "# max_value = predictions[max_emotion]\n",
        "# result = {max_emotion: max_value}\n",
        "# model_max_emotions.append(result)\n",
        "\n",
        "# print(\"âœ… Emotion condition: \", result)\n",
        "\n",
        "# prediction_response_json = predict_and_serialize(model_response, model_NCR_BERT, tokenizer_NCR_BERT)\n",
        "# # print(prediction_response_json)\n",
        "\n",
        "# prediction_json_load = json.loads(prediction_response_json)\n",
        "\n",
        "# # Get intensity values\n",
        "# intensities = prediction_json_load[\"predictions\"][\"intensity\"]\n",
        "\n",
        "# # Calculate overall score\n",
        "# overall_score = sum(\n",
        "#     intensities[emotion] * typical_valence.get(emotion, 0.0)\n",
        "#     for emotion in intensities\n",
        "# )\n",
        "\n",
        "# print(f\"âœ… Overall Emotion Score: {overall_score:.5f}\")\n",
        "# chat_model_history_score.append(overall_score)\n",
        "\n",
        "# # If streaming, you already saw output. Otherwise:\n",
        "# new_response = response[len(prompt):].strip()\n",
        "# chat_history.append({\"role\": \"assistant\", \"content\": new_response})\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "responses_score = []\n",
        "response_sentiment_score = []\n",
        "\n",
        "for x in responses:\n",
        "    predictions = predict_emotions(x)\n",
        "    max_emotion = max(predictions, key=predictions.get)\n",
        "    max_value = predictions[max_emotion]\n",
        "    result = {max_emotion: max_value}\n",
        "    model_max_emotions.append(result)\n",
        "\n",
        "    response_sentiment_score.append(list(result.values())[0])\n",
        "\n",
        "    # print(\"âœ… Emotion condition: \", result)\n",
        "\n",
        "    prediction_response_json = predict_and_serialize(x, model_NCR_BERT, tokenizer_NCR_BERT)\n",
        "    # print(prediction_response_json)\n",
        "\n",
        "    prediction_json_load = json.loads(prediction_response_json)\n",
        "\n",
        "    # Get intensity values\n",
        "    intensities = prediction_json_load[\"predictions\"][\"intensity\"]\n",
        "\n",
        "    # Calculate overall score\n",
        "    overall_score = sum(\n",
        "        intensities[emotion] * typical_valence.get(emotion, 0.0)\n",
        "        for emotion in intensities\n",
        "    )\n",
        "    responses_score.append(overall_score)\n",
        "\n",
        "    print(f\"âœ… Emotion condition:{result}   âœ… Overall Emotion Score: {overall_score:.5f}\")"
      ],
      "metadata": {
        "id": "2bBwkQm0dvPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(responses_score)\n",
        "print(response_sentiment_score)"
      ],
      "metadata": {
        "id": "hNNAwI1yhjmL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# min_val = min(responses_score)\n",
        "# max_val = max(responses_score)\n",
        "\n",
        "# # response_normalized_values = [(x - min_val) / (max_val - min_val) for x in responses_score]\n",
        "# response_normalized_values = []\n",
        "# for i in range(len(responses)):\n",
        "#     response_normalized_values.append((responses_score[i] - min_val) / (max_val - min_val))\n",
        "\n",
        "# print(\"âœ… Normalized values (0 to 1):\", response_normalized_values)"
      ],
      "metadata": {
        "id": "CY4ywkQ6pmBJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ranking_value = []\n",
        "alpha_score = 1\n",
        "for i in range(len(responses)):\n",
        "  ranking_score = ( 1- alpha_score ) * responses_score[i] + alpha_score * response_sentiment_score[i]\n",
        "  print(ranking_score)\n",
        "  ranking_value.append(ranking_score)\n",
        "\n",
        "\n",
        "max_value = max(ranking_value)\n",
        "max_index = ranking_value.index(max_value)\n",
        "\n",
        "print(\"âœ… Maximum Value:\", max_value)\n",
        "print(\"ðŸ”¢ Index of Maximum Value:\", max_index)"
      ],
      "metadata": {
        "id": "DlBC138wiz7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chat_model_history_score.append(responses_score[max_index])\n",
        "chat_history.append({\"role\": \"assistant\", \"content\": responses[max_index]})"
      ],
      "metadata": {
        "id": "Y7aYfoS7oU2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QEbtw6rXYYzg"
      },
      "source": [
        "**Next User message**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " responses[1]"
      ],
      "metadata": {
        "id": "TQxEfI2B589E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoBy43p2Yqp4"
      },
      "source": [
        "------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw3kKI2CYpgQ"
      },
      "outputs": [],
      "source": [
        "#----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "user_input = \"I just want someone to really understand me. Not fix me. Just understand\"\n",
        "\n",
        "\n",
        "#----------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "predictions = predict_emotions(user_input)\n",
        "max_emotion = max(predictions, key=predictions.get)\n",
        "max_value = predictions[max_emotion]\n",
        "result = {max_emotion: max_value}\n",
        "user_max_emotions.append(result)\n",
        "\n",
        "print(\"âœ… Emotion condition: \", result)\n",
        "\n",
        "chat_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "prediction_json = predict_and_serialize(user_input, model_NCR_BERT, tokenizer_NCR_BERT)\n",
        "prediction_json_load = json.loads(prediction_json)\n",
        "\n",
        "# Get intensity values\n",
        "intensities = prediction_json_load[\"predictions\"][\"intensity\"]\n",
        "\n",
        "# Calculate overall score\n",
        "overall_score = sum(\n",
        "    intensities[emotion] * typical_valence.get(emotion, 0.0)\n",
        "    for emotion in intensities\n",
        ")\n",
        "\n",
        "print(f\"âœ… Overall Emotion Score: {overall_score:.5f}\")\n",
        "chat_user_history_score.append(overall_score)\n",
        "\n",
        "MAX_TOKENS = 2048\n",
        "chat_prompt = tokenizer_BERT_Chat.apply_chat_template(chat_history, tokenize=False)\n",
        "while len(tokenizer_BERT_Chat(chat_prompt).input_ids) > MAX_TOKENS:\n",
        "    chat_history.pop(1)  # remove oldest user/assistant message (after system)\n",
        "    chat_prompt = tokenizer_BERT_Chat.apply_chat_template(chat_history, tokenize=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eERD3i7DfYiI"
      },
      "source": [
        "--------------------------------------------------------"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "o4QHBUrcNSZI",
        "vyK7M6j6OCbt",
        "1N61MUpCQ0wW",
        "lGAYucY8agZB"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}